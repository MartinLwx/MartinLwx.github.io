<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>线性回归模型指南 - 理论部分 - MartinLwx&#39;s blog</title><meta name="Description" content="机器学习中的线性回归模型指南，包括梯度下降算法推导等"><meta property="og:title" content="线性回归模型指南 - 理论部分" />
<meta property="og:description" content="机器学习中的线性回归模型指南，包括梯度下降算法推导等" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" /><meta property="og:image" content="https://martinlwx.github.io/logo.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-15T12:27:40+08:00" />
<meta property="article:modified_time" content="2023-03-15T12:27:40+08:00" /><meta property="og:site_name" content="MartinLwx&#39;s Blog" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://martinlwx.github.io/logo.png" /><meta name="twitter:title" content="线性回归模型指南 - 理论部分"/>
<meta name="twitter:description" content="机器学习中的线性回归模型指南，包括梯度下降算法推导等"/>
<meta name="application-name" content="我的网站">
<meta name="apple-mobile-web-app-title" content="我的网站"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" /><link rel="prev" href="https://martinlwx.github.io/zh-cn/config-neovim-from-scratch/" /><link rel="next" href="https://martinlwx.github.io/zh-cn/backpropagation-tutorial/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "线性回归模型指南 - 理论部分",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/martinlwx.github.io\/zh-cn\/linear-regression-model-guide-theory\/"
        },"genre": "posts","keywords": "Machine-Learning","wordcount":  4566 ,
        "url": "https:\/\/martinlwx.github.io\/zh-cn\/linear-regression-model-guide-theory\/","datePublished": "2023-03-15T12:27:40+08:00","dateModified": "2023-03-15T12:27:40+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "MartinLwx"
            },"description": "机器学习中的线性回归模型指南，包括梯度下降算法推导等"
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/zh-cn/" title="MartinLwx&#39;s blog"></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/zh-cn/"> 主页 </a><a class="menu-item" href="/zh-cn/posts/"> 文章 </a><a class="menu-item" href="/zh-cn/tags/"> 标签 </a><a class="menu-item" href="/zh-cn/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a><a href="javascript:void(0);" class="menu-item language" title="选择语言">
                    <i class="fa fa-globe" aria-hidden="true"></i>                      
                    <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/en/linear-regression-model-guide-theory/">English</option><option value="/zh-cn/linear-regression-model-guide-theory/" selected>简体中文</option></select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/zh-cn/" title="MartinLwx&#39;s blog"></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/zh-cn/" title="">主页</a><a class="menu-item" href="/zh-cn/posts/" title="">文章</a><a class="menu-item" href="/zh-cn/tags/" title="">标签</a><a class="menu-item" href="/zh-cn/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">
                    <i class="fa fa-globe fa-fw" aria-hidden="true"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/en/linear-regression-model-guide-theory/">English</option><option value="/zh-cn/linear-regression-model-guide-theory/" selected>简体中文</option></select>
                </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">线性回归模型指南 - 理论部分</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/zh-cn/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>MartinLwx</a></span>&nbsp;<span class="post-category">收录于 <a href="/zh-cn/categories/ml-dl/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>ML-DL</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2023-03-15">2023-03-15</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 4566 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 10 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="true">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#引言">引言</a></li>
    <li><a href="#线性回归模型">线性回归模型</a></li>
    <li><a href="#分类">分类</a>
      <ul>
        <li><a href="#单变量线性回归">单变量线性回归</a></li>
        <li><a href="#多变量线性回归">多变量线性回归</a></li>
      </ul>
    </li>
    <li><a href="#模型如何训练">模型如何训练</a>
      <ul>
        <li><a href="#损失函数">损失函数</a></li>
        <li><a href="#梯度下降训练法">梯度下降训练法</a></li>
      </ul>
    </li>
    <li><a href="#总结">总结</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h2 id="引言">引言</h2>
<p>最近，重新刷起了吴恩达的机器学习课程，系统性复习了之前学过的知识，发现又有不少收获，打算仔细整理一番👍</p>
<p>要谈论什么是线性回归首先要对什么是机器学习有一个基本的认识，什么是机器学习？抽象来说机器学习就是学习一个函数
$$
f(input) = output
$$
其中 $f$ 指的就是具体的机器学习模型。<strong>机器学习就是自动拟合输入 - 输出之间的关系</strong>的一套方法论。有时候我们会发现一些问题很难定义出一个具体的算法来解决，这时就是机器学习发光发热的地方了，我们可以让它从数据中自己学习、总结一些模式，做出相关的预测。这也是它和传统的算法（二分、递归等）区别的地方。不得不承认，机器学习从定义上来说就很迷人，它<em>似乎</em>为所有难以解决的问题提供了一套可行的解决框架。恰恰现实生活中的一些问题就是很难用传统算法解决的</p>
<blockquote>
<p>📒 为此，我总觉得<strong>每个程序员都应该懂点机器学习/深度学习</strong>，它也是我们解决问题的一大工具💪</p>
</blockquote>
<p>线性回归就是经典的机器学习入门模型之一，可以说是机器学习界的 <a href="https://en.wikipedia.org/wiki/%22Hello,_World!%22_program" target="_blank" rel="noopener noreffer ">“Hello world”</a>，<em>经典的比如波士顿房价预测项目（虽然说现在已经是烂大街的项目了）</em></p>
<p>前面的公式中，$input$ 为机器学习中所谓的特征（Feature），常用记号 $x$ 表示。而 $output$ 可以根据是预测值还是预测类别可以大致划分为回归问题（Regression problem）和分类问题（Classification problem）两大类</p>
<blockquote>
<p>📒 如何理解特征？所谓<strong>特征就是，跟要预测的东西高度相关的东西</strong>。<em>还是用预测房价作为例子，房价显然跟占地面积、绿化条件等相关，这里的占地面积等就是特征</em>。机器学习中可以对特征分析，看哪些跟输出的关联程度高，或者运用我们的领域知识（Domain knowledge），自己选择好的特征。但总的来说，<strong>机器学习还是要求我们花费大量精力在提取好的特征上</strong>，这也是机器学习被人诟病的地方，而这个缺点在深度学习中被大大缓解，当然那是后话了</p>
</blockquote>
<blockquote>
<p>📒 机器学习<strong>无法魔法般地理解</strong>你提供的各种格式的 $input$，<em>比如图片、文本、视频等</em>。在机器学习中，<strong>$input$ 常常被处理为数字</strong>，才能用各种机器学习方法学习。有些特征本身就是数字，<em>比如预测房价，房子的占地面积这个特征本身就是一个数字</em>。当输入不是数字的时候，就需要别的手段将输入转化为数字，<em>比如用词嵌入向量模型表示文本等</em>，这里不展开细讲。<strong>我们暂时假设已经将输入处理为了数字的形式</strong></p>
</blockquote>
<p>今天要提到的线性回归模型就属于<strong>监督学习分类下的回归模型</strong>，它足够简单，但是又可以阐述很多机器学习的思想，用来入门是再适合不过了。话不多说，让我们开始吧 :)</p>
<blockquote>
<p>📒 本文假定你对基本的微积分和线性代数有所了解，<em>比如你需要知道行向量乘列向量如何进行以及对应的记号表示，矩阵乘法的定义，函数如何求导等</em></p>
</blockquote>
<h2 id="线性回归模型">线性回归模型</h2>
<p>线性回归的英文是 Linear regression。顾名思义，由两部分组成：</p>
<ul>
<li>Linear 的含义就是它<strong>是线性的</strong>。<em>以二维平面为例，$y=kx+b$ 这种就是线性，画出来是一条直线，而 $y=x^2$ 这种就不是，因为它画出来为曲线</em></li>
<li>Regression 是因为它符合<strong>机器学习中对回归问题的定义</strong>——预测一个<strong>不限制范围</strong>的值</li>
</ul>
<h2 id="分类">分类</h2>
<blockquote>
<p>📒 为了形式的简洁下面的记号 $f(x)$ 其实省略了下标 $w,b$。$f(x)=f_{w,b}(x)$</p>
</blockquote>
<blockquote>
<p>📒 有时候会看到有的书或者博客用 $h_\theta$ 表示模型，我认为 $f(x)$ 比较简洁就用了这个记法</p>
</blockquote>
<p>如果要进一步对线性回归模型进行细分，又可以分成如下几类</p>
<h3 id="单变量线性回归">单变量线性回归</h3>
<p>如果模型的输入特征只有一个，就叫做单变量线性回归（Univariate linear regression），此时达到了最简单的形式
$$
f(x)=wx+b
$$
在机器学习中，分别称 $w$ 和 $b$ 为权重（Weight）和偏置项（Bias）</p>
<blockquote>
<p>📒 注意：$\theta$ 为一个向量，按道理应该用 $\vec \theta$ 表示，但是下面为了简洁，我都省略了箭头</p>
</blockquote>
<blockquote>
<p>📒 也可以用向量的形式写，用 $\theta=[b, w]$ 表示模型的参数，令 $\vec x=[1, x]^T$。那么根据向量乘法的知识我们可以知道 $\theta^T\vec x=wx+b$</p>
</blockquote>
<h3 id="多变量线性回归">多变量线性回归</h3>
<p>如果模型有多个输入的特征，就叫做多变量线性回归（Multiple linear regresseion）
$$
f(x)=w_1x_1+w_1x_2+&hellip;w_nx_n+b
$$
其中 $x_i$ 是不同的特征，一共有 $n$ 个特征，为此我们需要每个特征学习一个权重 $w_i$</p>
<blockquote>
<p>📒 同理，可以选择用向量的形式来写，令 $\theta=[b, w_1, w_2, &hellip;, w_n]$，$\vec x=[1, x_1, x_2, &hellip;, x_n]^T$，向量乘法之后也可以得到上面的形式。你会惊讶地发现，<strong>单变量线性回归和多变量线性回归有了统一的形式——$f(\vec x)=\theta^T\vec x$</strong>。<strong>这在后面求解梯度的时候会带来很大的方便</strong></p>
</blockquote>
<h2 id="模型如何训练">模型如何训练</h2>
<h3 id="损失函数">损失函数</h3>
<p>定义了线性回归模型之后，我们需要衡量它的预测好坏，显然，我们<strong>希望预测的值跟实际上的值的“距离”越接近越好</strong>。在机器学习中，我们会规定一个损失函数（Cost function）衡量“距离”，<strong>损失自然是越低越好</strong>。当模型在验证集上的损失降到最低不再明显变化的时候，我们认为模型收敛了，此时得到了最好的模型</p>
<blockquote>
<p>📒 在机器学习中，通常将数据划分为训练集/验证集/测试集，模型用训练集上的数据学习，在验证集上验证、调参，最后在测试集上进行泛化性验证（<strong>测试集包含没有见过的数据</strong>）。只用训练集/测试集这种划分方法，直接在测试集上调参<strong>是不正确的做法</strong>。测试集<strong>当且仅当</strong>最后你训练和调参结束，选出你认为的最好模型的的时候，在论文里面是报告结果的时候采用的</p>
</blockquote>
<p>线性回归模型的损失函数最常用的是均平方误差，我更经常直接用英文的缩写 MSE（Mean square error）代指。其公式如下：</p>
<p>$$
MSE=\frac{1}{2m}\sum^m_{i=1}(\hat y^{(i)} - y^{(i)})^2
$$</p>
<p>其中</p>
<ul>
<li>$m$ 表示样本个数</li>
<li>上角标 ${(i)}$ 加了括号，和指数的记号进行区分，表示第 $i$ 个样本的预测/真实值</li>
<li>$\hat y$ 这个记号表示线性回归模型的预测，$y$ 表示本来的真实值</li>
</ul>
<blockquote>
<p>📒 我采用了符合机器学习惯例的记号，推荐记住～</p>
</blockquote>
<blockquote>
<p>📒 线性回归模型<strong>不是非要</strong>用 MSE，也可以用绝对值误差 MAE，MAE 适用于：当数据集中的异常值（Outlier）比较多的时候，降低对这种样本的敏感度</p>
</blockquote>
<h3 id="梯度下降训练法">梯度下降训练法</h3>
<p>前面提到当模型在验证集上的损失降到最低而且不再变化的时候，我们得到最好的模型。但是要如何让模型不断改进预测，降低损失呢？🤔 梯度下降（Gradient descent）算法是一种通用的做法</p>
<p>不要被这个名字所迷惑，梯度下降算法其实没有那么神秘。先整理我们目前学到了什么：</p>
<ul>
<li>模型的预测 $\hat y$ 跟它参数，<em>如果是单变量线性回归模型，就是和 $w$ 和 $b$ 有关</em></li>
<li>损失函数跟模型的预测有关系，因为数据本身真实的值这个我们是无法改变的</li>
</ul>
<p>所以<strong>损失函数实际上是一个关于模型参数的函数</strong>，在机器学习中，常用记号 $J$ 表示</p>
<p>先考虑简单的单变量线性回归，当它采用 MSE 作为损失函数的时候：
$$
J(w, b) = \frac{1}{2m}\sum^m_{i=1}(f(x^{(i)})-y^{(i)})^2 = \frac{1}{2m}\sum^m_{i=1}(wx^{(i)} + b - y^{(i)})^2
$$</p>
<p><strong>根据 $w$ 和 $b$ 的取值不同，$J(w,b)$ 也不同</strong>，因此<strong>改进线性回归模型其实就是一直在调整这两个参数让 $J(w,b)$ 的值更小</strong>。用数学的语言来说，求解最优模型其实就是求解函数 $J(w,b)$ 的最小点</p>
<blockquote>
<p>📒 回忆 $m$ 为训练集的样本个数。更为严格来说，上面的梯度下降式子是批梯度下降（Batch gradient descent），即<strong>每个计算梯度的时候用的是整个训练集上的样本</strong>。当数据集很大的时候这个方法就无法很好 Scale，此时我们就需要使用随机梯度下降（Stochastic gradient descent）</p>
</blockquote>
<blockquote>
<p>📒 只要学过导数、最小值，我们不难求解出上面公式的最小值，但<strong>这是因为线性回归模型比较简单</strong>，当模型更加复杂的时候，用数学方法求解也会更加困难，因此<strong>在实践中采用的都是梯度下降算法</strong>（不考虑强化学习，因为强化学习优化的目标一般不是一个可微的函数）</p>
</blockquote>
<p>让我们<strong>暂时不考虑 $b$ 只考虑 $w$</strong>，那么此时 $J$ 是一个关于 $w$ 的函数，我们可以以 $w$ 为横轴，$J(w)$ 为纵轴画出如下的图：</p>
<figure><img src="/img/linear_regression_tagent.png" width="70%"/>
</figure>

<blockquote>
<p>📒 注意，上面的图并不严格遵循 $J(w)$ 的定义，为了方便我这里直接画了 $y=\frac{1}{2}x^2$。但是<strong>形状应该差不多</strong>，可以用来理解梯度下降算法</p>
</blockquote>
<p>从图中不难看出，当 $w=0$ 的时候 $J(w)$ 取到了最小值。假设模型当前的参数 $w=5$，计算得到损失 $J(5)=12.5$，我们要如何更新 $w$ 让模型更好呢？<strong>答案是让 $w$ 沿着梯度的</strong>反方向<strong>更新（图中红色的线为在 $w=5$ 这个点的切线）</strong>，不难求出这一点的梯度（导数）是 $5$，$w$ 应该减小，因此应该是减去这个梯度（所以说是反方向），同时引入<strong>学习率 $\alpha$ 控制更新的幅度</strong>，得到更新式子 $w \leftarrow w - \alpha \cdot 5$</p>
<blockquote>
<p>📒 想象你自己站在 $w=5$ 这个点要前进到 $w=0$ 这个最小值在的点，如果学习率 $\alpha$ 太大，你新得到的 $w$ 可能会小于 $0$，一下子越过了 $w=0$ 这个点。虽然你可以通过梯度下降再次尝试更新，但此时往往你会发现你的模型的损失一直在变化无法收敛。<strong>学习率和梯度共同控制了每次参数更新的幅度</strong></p>
</blockquote>
<blockquote>
<p>📒 我们在只考虑 $w$ 的时候得出了该如何更新的结论——沿着梯度的反方向，<strong>考虑更多的参数的时候这个结论仍然适用</strong>，不过那就要涉及到求解偏导数乃至向量求导的知识，而且此时的 $J$ 的可视化也更为困难，所以这里不细讲。<strong>只需要记住模型的更新总是沿着梯度的反方向前进，从直觉上进行把握</strong>👻</p>
</blockquote>
<p>在单变量线性回归中，梯度更新的式子如下：
$$
w \leftarrow w - \alpha \cdot \frac{\partial}{\partial w}J(w,b)
$$</p>
<p>别忘了还有 $b$，它也要更新
$$
b \leftarrow b - \alpha \cdot \frac{\partial}{\partial b}J(w,b)
$$</p>
<p>我们来对其中的梯度计算进行推导
$$
\begin{aligned}
\frac{\partial}{\partial w}J(w,b)&amp;=\frac{\partial}{\partial w}\frac{1}{2m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2 \\\
&amp;= \frac{\partial}{\partial w}\frac{1}{2m}(wx^{(i)}+b-y^{(i)})^2 \\\
&amp;= \frac{\partial}{\partial w}\frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})x^{(i)}
\end{aligned}
$$
$$
\begin{aligned}
\frac{\partial}{\partial b}J(w,b)&amp;=\frac{\partial}{\partial w}\frac{1}{2m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})^2 \\\
&amp;= \frac{\partial}{\partial b}\frac{1}{2m}(wx^{(i)}+b-y^{(i)})^2 \\\
&amp;= \frac{\partial}{\partial b}\frac{1}{m}\sum_{i=1}^m(f(x^{(i)})-y^{(i)})
\end{aligned}
$$</p>
<p>如果是多变量线性回归，我们要求解每个 $\frac{\partial }{w_i}J(w_1,w_2, &hellip;,b)$ 显然很不方便，<strong>此时用向量形式推导梯度是最好的</strong></p>
<p>在向量的形式下，损失函数 MSE 写作：</p>
<p>$$
J(\theta) = \frac{1}{2m}(X\theta - \vec{y})^T(X\theta - \vec{y})
$$</p>
<p>其中</p>
<ul>
<li>$X$ 为输入的矩阵，<strong>常用大写字母表示矩阵</strong>，它的每一行为一个样本的特征值 $(x^{(i)})^T$，大小为 $(m, n+1)$
<ul>
<li>前面提到我们有 $n$ 个特征，这里为 $n+1$ 维是因为在第一个位置插入了一个 $1$，这样相乘的时候 $1$ 会和 $b$ 做计算</li>
<li>$x^{(i)}$ 需要转置因为本来它本是列向量</li>
<li>对于单个样本是 $\theta^T\vec x$，对于 $m$ 个样本的计算就要用矩阵乘法 $X\theta$</li>
</ul>
</li>
<li>$\theta$ 正如我们前面说的是 $[b, w_1, w_2, &hellip;, w_n]$，长为 $n + 1$</li>
<li>因此 $X\theta$ 就是模型的预测值，根据矩阵乘法的知识，我们会得到长为 $m$ 的向量，表示对每个样本的预测</li>
<li>$X\theta - \vec y$ 就是每个预测的误差</li>
<li>顺带一提，假设 $\vec a$ 为列向量，<strong>$\vec a^T\vec a$ 得到的总是一个标量，为每个元素的平方和</strong>。如果让 $\vec a=X\theta -\vec y$ 就得到了上面右边的部分，这也解释了为什么会是这个形式</li>
</ul>
<blockquote>
<p>🤔️ 更新：在 <a href="https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/" rel="">维度分析</a> 这篇博客中，我展示了如何用维度分析技巧<strong>快速</strong>推导出这个公式的解，感兴趣的话可以看下</p>
</blockquote>
<p>下面我们开始尝试推导这个公式的梯度，<strong>这需要你有一定的向量/矩阵求导知识，可以选择跳过🔮</strong>。<del>不过我建议还是看看，因为机器学习里面还挺多公式推导的</del></p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \theta}\ J(w,b)
&amp;= \frac{\partial}{\partial \theta}\ \frac{1}{2m}(X\theta - \vec{y})^T(X\theta - \vec{y}) \\\
&amp;= \frac{1}{2m}\frac{\partial}{\partial \theta}\ (\theta^TX^T - \vec{y}^T)(X\theta - \vec{y}) \\\
&amp;= \frac{1}{2m}\frac{\partial}{\partial \theta}\ (\theta^TX^TX\theta - \theta^TX^T\vec y - \vec y^TX\theta + \vec y^T\vec y) \\\
&amp;= \frac{1}{2m}\frac{\partial}{\partial \theta}\ (\theta^TX^TX\theta - \theta^T(X^T\vec y) - (X^T\vec y)^T\theta + \vec y^T\vec y) \\\
&amp;= \frac{1}{2m}\frac{\partial}{\partial \theta}\ (\theta^TX^TX\theta - 2\theta^T(X^T\vec y) + \vec y^T\vec y) \\\
&amp;= \frac{1}{2m}(\frac{\partial}{\partial \theta}\ (\theta^TX^TX\theta) - 2\frac{\partial}{\partial \theta}(\theta^TX^T\vec y))) \\\
&amp;= \frac{1}{2m}(2X^TX\theta - 2X^T\vec y)) \\\
&amp;= \frac{1}{m}(X^TX\theta - X^T\vec y)) \\\
&amp;= \frac{1}{m}X^T(X\theta-\vec y)
\end{aligned}
$$</p>
<p>几个解释：</p>
<ul>
<li>$X^T$ 的大小是 $(n+1, m)$，$\vec y$ 是 $(m, 1)$，因此 $X^T\vec y$ 其实是一个维度为 $(n+1, 1)$ 的列向量，<strong>注意在线性代数中，当 $\vec a$ 和 $\vec b$ 都为列向量的时候，有 $\vec a^T\vec b=\vec b^T\vec a$ 成立</strong>。所以 $\theta^T(X^T\vec y) = (X^T\vec y)^T\theta$</li>
<li>其他几个地方的推导我推荐直接代向量/矩阵求导公式，可以参考这个 <a href="http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf" target="_blank" rel="noopener noreffer ">Cheatsheet</a>。当然如果对这方面很感兴趣更推荐直接去看原理证明
<ul>
<li>比如上面的 $\frac{\partial}{\partial \theta}(\theta^TX^T\vec y))$，其实就是换元法，令 $\vec b=X^T\vec y$，刚好对应 Cheatsheet 里面的 $\frac{\partial }{\partial \theta}\theta^T\vec b=\vec b$</li>
</ul>
</li>
</ul>
<blockquote>
<p>📒 向量形式在实现代码的时候也带来了很大方便，我们可以利用成熟的 <a href="https://numpy.org/" target="_blank" rel="noopener noreffer ">Numpy</a> <strong>高效</strong>进行计算，而不是自己写一个 for 循环一个个样本计算</p>
</blockquote>
<h2 id="总结">总结</h2>
<p>线性回归模型的理论部分就是以上的内容，本文介绍了单变量线性回归，多变量线性回归，前者可以看成是后者的一个特例。最后我们将其都用向量的形式统一了写法，并给出了向量形式下采用 MSE 作为损失函数的时候的梯度计算公式</p>
<p>本来还想放一下代码在这里，结果写着写着发现这一篇博客已经很长了，看来只好将线性回归模型的算法放在另一篇了🙌</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2023-03-15</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" data-title="线性回归模型指南 - 理论部分" data-hashtags="Machine-Learning"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" data-hashtag="Machine-Learning"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" data-title="线性回归模型指南 - 理论部分"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" data-title="线性回归模型指南 - 理论部分"><i data-svg-src="/lib/simple-icons/icons/line.min.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/" data-title="线性回归模型指南 - 理论部分"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/zh-cn/tags/machine-learning/">Machine-Learning</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/zh-cn/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/zh-cn/config-neovim-from-scratch/" class="prev" rel="prev" title="从零开始配置 Neovim(Nvim)"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>从零开始配置 Neovim(Nvim)</a>
            <a href="/zh-cn/backpropagation-tutorial/" class="next" rel="next" title="反向传播公式推导和理解">反向传播公式推导和理解<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.121.2">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/zh-cn/" target="_blank">MartinLwx</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/zh-cn/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-7RY6742J2F', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-7RY6742J2F" async></script></body>
</html>
