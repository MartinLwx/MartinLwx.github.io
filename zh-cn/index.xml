<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MartinLwx&#39;s blog</title>
        <link>https://martinlwx.github.io/zh-cn/</link>
        <description>Welcome to my blog :)</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><lastBuildDate>Sat, 01 Jul 2023 17:12:40 &#43;0800</lastBuildDate>
            <atom:link href="https://martinlwx.github.io/zh-cn/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>如何记忆红黑树的操作</title>
    <link>https://martinlwx.github.io/zh-cn/how-to-memorize-insertion-and-deletion-in-rb-tree/</link>
    <pubDate>Sat, 01 Jul 2023 17:12:40 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/how-to-memorize-insertion-and-deletion-in-rb-tree/</guid>
    <description><![CDATA[引言 如果你点进了这一篇文章，相信你也跟我一样：红黑树学一次忘一次，又要做树的旋转，又要给节点重新上色，导致每次都是学完了就忘记。我也曾经仔细]]></description>
</item>
<item>
    <title>Git Bundle 指南</title>
    <link>https://martinlwx.github.io/zh-cn/git-bundle-tutorial/</link>
    <pubDate>Fri, 16 Jun 2023 23:48:28 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/git-bundle-tutorial/</guid>
    <description><![CDATA[git bundle 是什么 git bundle 是一个比较少看到的 git 命令，它的作用是把一个 git 仓库打包📦成一个文件，然后别人可以通过这个文件还原出本来的 git 仓库，而且 git bundle 还支持增]]></description>
</item>
<item>
    <title>用 MPNN 框架解读 GAT</title>
    <link>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</link>
    <pubDate>Sun, 21 May 2023 15:20:50 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</guid>
    <description><![CDATA[什么是 MPNN 框架 Justin Gilmer 提出了 MPNN（Message Passing Neural Network）框架1 ，用于描述被用来做图上的监督学习的图神经网络模型。我发现这是一个很好]]></description>
</item>
<item>
    <title>SICP 练习 2.27</title>
    <link>https://martinlwx.github.io/zh-cn/sicp-exercise-2-27/</link>
    <pubDate>Tue, 16 May 2023 12:41:20 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/sicp-exercise-2-27/</guid>
    <description><![CDATA[题目 Modify your reverse procedure of exercise 2.18 to produce a deep-reverse procedure that taks a list as argument and returns as its value the list with its elements reversed and with all sublists deep-reversed as well. 1 2 3 4 (define x (list (list 1 2) (list 3 4))) ;; x - ((1 2) (3 4)) (deep-reverse x) ;; the output should be ((4 3) (2 1)) 答]]></description>
</item>
<item>
    <title>SICP 练习 1.46</title>
    <link>https://martinlwx.github.io/zh-cn/sicp-exercise-1-46/</link>
    <pubDate>Wed, 10 May 2023 13:40:48 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/sicp-exercise-1-46/</guid>
    <description><![CDATA[Question Several of the numerical methods described in this chapter are instances of an extremely general computational strategy known as iterative improvement. Iterative improvement says that, to compute something, we start with an initial guess for the answer, test if the guess is good enough, and otherwise improve the guess and continue the process using the improved guess as the new guess. Write a procedure iterative-improve that takes two procedures]]></description>
</item>
<item>
    <title>SICP 练习 1.34</title>
    <link>https://martinlwx.github.io/zh-cn/sicp-exercise-1-34/</link>
    <pubDate>Tue, 09 May 2023 14:06:18 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/sicp-exercise-1-34/</guid>
    <description><![CDATA[题目 Suppose we define the procedure f. What happens if we (perversely) ask the interpreter to evaluate the combination (f f)? 1 2 3 4 5 (define (square x) (* x x)) (define (f g) (g 2)) Then we have 1 2 3 (f square) ;; 4 (f (lambda (z) (* z (+ z 1)))) ;; 6 = 2 * 3 答案 回忆之前书上]]></description>
</item>
<item>
    <title>CS61A 的项目四之 Scheme 解释器实现 (2021-Fall)</title>
    <link>https://martinlwx.github.io/zh-cn/proj4-scheme-interpreter-of-cs61a-of-ucb-fa21/</link>
    <pubDate>Fri, 21 Apr 2023 10:18:16 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/proj4-scheme-interpreter-of-cs61a-of-ucb-fa21/</guid>
    <description><![CDATA[引言 最近正在跟着《Crafting interpreter》这本书写解释器，原本书里面用 Java 实现了一个 Tree-walker 解释器 jlox，我正在用 Python 重写一遍，称为]]></description>
</item>
<item>
    <title>用 SRTBOT 框架分析动态规划问题</title>
    <link>https://martinlwx.github.io/zh-cn/solving-dynamic-programming-problems-using-srtbot/</link>
    <pubDate>Sun, 09 Apr 2023 12:30:31 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/solving-dynamic-programming-problems-using-srtbot/</guid>
    <description><![CDATA[Changelog: 更新依赖图 @2023.04.13 引言 在做算法题的时候，让我头疼的经常是动态规划问题，它属于那种自己琢磨半天想不出来，但是看了答案之后会恍然大悟，下次再做的话很]]></description>
</item>
<item>
    <title>反向传播公式推导和理解</title>
    <link>https://martinlwx.github.io/zh-cn/backpropagation-tutorial/</link>
    <pubDate>Tue, 04 Apr 2023 13:45:48 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/backpropagation-tutorial/</guid>
    <description><![CDATA[引言 在深度学习中，模型的优化是通过采用梯度下降法不断更新权重和偏置项，让损失越来越小。其中的核心就是反向传播算法。回忆梯度下降的公式，用 $\theta$ 表]]></description>
</item>
<item>
    <title>线性回归模型指南 - 理论部分</title>
    <link>https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/</link>
    <pubDate>Wed, 15 Mar 2023 12:27:40 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/</guid>
    <description><![CDATA[引言 最近，重新刷起了吴恩达的机器学习课程，系统性复习了之前学过的知识，发现又有不少收获，打算仔细整理一番👍 要谈论什么是线性回归首先要对什么是]]></description>
</item>
</channel>
</rss>
