<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/</link>
        <description>Welcome to my blog :)</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Mon, 29 Jul 2024 23:35:53 &#43;0800</lastBuildDate>
            <atom:link href="https://martinlwx.github.io/zh-cn/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>使用 GitHub Actions 自动化 Hugo 博客部署</title>
    <link>https://martinlwx.github.io/zh-cn/use-github-actions-to-automated-hugo-build/</link>
    <pubDate>Mon, 29 Jul 2024 23:35:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/use-github-actions-to-automated-hugo-build/</guid>
    <description><![CDATA[前言最近在学习 GitHub Actions，GitHub Actions 是 GitHub 提供的一个特性，可以用来自动化执行一些步骤。在软件开发中，最常见的需要自动化的场景可能就是]]></description>
</item><item>
    <title>尾调用与尾调用优化</title>
    <link>https://martinlwx.github.io/zh-cn/tail-call-and-tail-call-optimization/</link>
    <pubDate>Fri, 22 Mar 2024 12:13:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/tail-call-and-tail-call-optimization/</guid>
    <description><![CDATA[尾调用 &amp; 尾递归假设函数 A 调用了函数 B，我们称函数 A 为 Caller，函数 B 为 Callee。 尾调用（Tail-call）指的是：Caller 最后]]></description>
</item><item>
    <title>学习使用 Vim&amp;Neovim 的 text-object</title>
    <link>https://martinlwx.github.io/zh-cn/learn-to-use-text-objects-in-vim/</link>
    <pubDate>Sun, 03 Mar 2024 19:46:48 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/learn-to-use-text-objects-in-vim/</guid>
    <description><![CDATA[引言你可能不知道什么是 text-object，但我相信你可能已经在使用了只是你自己没有意识到。比如，在写代码的时候，我们经常想要修改函数调用]]></description>
</item><item>
    <title>OCaml 的 Neovim 配置方案</title>
    <link>https://martinlwx.github.io/zh-cn/neovim-setup-for-ocaml/</link>
    <pubDate>Tue, 23 Jan 2024 00:03:55 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/neovim-setup-for-ocaml/</guid>
    <description><![CDATA[从零开始用 Neovim 配置 OCaml 的开发环境，包括 LSP、Formatter 等]]></description>
</item><item>
    <title>LLM 推理加速 - KV Cache</title>
    <link>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 16:38:18 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[背景LLM 用于推理的时候就是不断基于前面的所有 token 生成下一个 token 假设现在已经生成了 $t$ 个 token，用 $x_{1:t}$ 表示。在下一轮，LLM 会生成 $x_{1:t]]></description>
</item><item>
    <title>LoRA 微调</title>
    <link>https://martinlwx.github.io/zh-cn/lora-finetuning/</link>
    <pubDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/lora-finetuning/</guid>
    <description><![CDATA[什么是 LoRA 自从 LLM 时代到来之后，如何微调 LLM 成为了一个难题，因为 LLM 的模型实在是太大了，很难做全量微调更新所有参数。可选的路线有：冻结整个模型做 Prompt tuning]]></description>
</item><item>
    <title>下一个排列问题</title>
    <link>https://martinlwx.github.io/zh-cn/the-next-lexicographical-permutation-problem/</link>
    <pubDate>Wed, 06 Sep 2023 23:13:52 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/the-next-lexicographical-permutation-problem/</guid>
    <description><![CDATA[引言有时候我们会想要生成一个序列的「下一个排列」或者是「上一个排列」，你会怎么做呢？如果你对 C++ 很熟悉的话，不难想到可以用 next_permutation1 和 prev_per]]></description>
</item><item>
    <title>BPE 分词解密 - 实现方法与示例讲解</title>
    <link>https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/</link>
    <pubDate>Thu, 24 Aug 2023 22:06:37 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/</guid>
    <description><![CDATA[BPE 简介在 NLP 里面，一个核心的问题是，如何对文本进行分词？从分类的角度上面来说，可以分为： Char level Word level Subword level 先看 Char level 分词，顾名思义，就是把文本拆分成一]]></description>
</item><item>
    <title>TF-IDF 模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[什么是 TF-IDF 模型在之前的文章中谈到了词袋模型，也讲到了它的许多不足，在今天的这篇文章中，我们要尝试解决词袋模型的缺点之一：每个词的重要性是一样的]]></description>
</item><item>
    <title>词袋模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[什么是词袋模型在 NLP 中，我们需要将文档（document）表示为向量，这是因为机器学习只能够处理数字。也就是说，我们要找到下面这么一个神奇的函]]></description>
</item></channel>
</rss>
