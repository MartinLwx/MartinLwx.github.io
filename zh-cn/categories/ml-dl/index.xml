<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>ML-DL - 分类 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/categories/ml-dl/</link>
        <description>ML-DL - 分类 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Thu, 12 Oct 2023 16:38:18 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/categories/ml-dl/" rel="self" type="application/rss+xml" /><item>
    <title>LLM 推理加速 - KV Cache</title>
    <link>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 16:38:18 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[背景 LLM 用于推理的时候就是不断基于前面的所有 token 生成下一个 token 假设现在已经生成了 $t$ 个 token，用 $x_{1:t}$ 表示。在下一轮，LLM 会生成 $x_{1:t+1]]></description>
</item>
<item>
    <title>LoRA 微调</title>
    <link>https://martinlwx.github.io/zh-cn/lora-finetuning/</link>
    <pubDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/lora-finetuning/</guid>
    <description><![CDATA[什么是 LoRA 自从 LLM 时代到来之后，如何微调 LLM 成为了一个难题，因为 LLM 的模型实在是太大了，很难做全量微调更新所有参数。可选的路线有：冻结整个模型做 Prompt tuning]]></description>
</item>
<item>
    <title>机器学习求解梯度的小技巧</title>
    <link>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</link>
    <pubDate>Wed, 26 Jul 2023 00:31:50 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</guid>
    <description><![CDATA[引言 也许你和我一样在求解机器学习的梯度时有各种困难，即使看着相关的 Cookbook 一边推导依然是有困惑，今天我要分享的是最近学习到的一个实用技巧：在机器学]]></description>
</item>
<item>
    <title>Pytorch 张量的 strides 格式是什么</title>
    <link>https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/</link>
    <pubDate>Fri, 14 Jul 2023 15:26:16 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/</guid>
    <description><![CDATA[引言 尽管我已经使用 Numpy 和 Pytorch 好长一段时间了，但我一直不知道他们是如何实现底层的张量（tensor），而且这么高效。最近在看 Deep Learning Systems 这门课，终于有机]]></description>
</item>
<item>
    <title>反向传播公式推导和理解</title>
    <link>https://martinlwx.github.io/zh-cn/backpropagation-tutorial/</link>
    <pubDate>Tue, 04 Apr 2023 13:45:48 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/backpropagation-tutorial/</guid>
    <description><![CDATA[更新：矩阵形式的反向传播可以看 这里 引言 在深度学习中，模型的优化是通过采用梯度下降法不断更新权重和偏置项，让损失越来越小。其中的核心就是反向传]]></description>
</item>
<item>
    <title>线性回归模型指南 - 理论部分</title>
    <link>https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/</link>
    <pubDate>Wed, 15 Mar 2023 12:27:40 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/linear-regression-model-guide-theory/</guid>
    <description><![CDATA[引言 最近，重新刷起了吴恩达的机器学习课程，系统性复习了之前学过的知识，发现又有不少收获，打算仔细整理一番👍 要谈论什么是线性回归首先要对什么是]]></description>
</item>
</channel>
</rss>
