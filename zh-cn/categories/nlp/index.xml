<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - 分类 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/categories/nlp/</link>
        <description>NLP - 分类 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Sat, 30 Nov 2024 00:01:26 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/categories/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>RAG 技术之 REALM</title>
    <link>https://martinlwx.github.io/zh-cn/rag-realm-paper-reading/</link>
    <pubDate>Sat, 30 Nov 2024 00:01:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/rag-realm-paper-reading/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>最近打算系统性学习 RAG 技术，开始看起了相关文献，目前的思路是按照 ACL 2023 Tutorial 的 <a href="https://acl2023-retrieval-lm.github.io/slides/3-architecture.pdf" target="_blank" rel="noopener noreferrer">Roadmap</a> 过一遍。本篇是对早期的 RAG 技术的 REALM 的介绍</p>
<div class="details admonition info open">
    <div class="details-summary admonition-title">
        <span class="icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></svg></span>Info<span class="details-icon"><svg class="icon"
    xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></span>
    </div>
    <div class="details-content">
        <div class="admonition-content"><p>本文采用的模型是 Masked LM 的 BERT，还不是 LLM。因此本文后续的部分内容需要你<em>对 BERT 有一定的了解</em>，包括 BERT 的预训练过程、BERT 微调等</p>]]></description>
</item><item>
    <title>BPE 分词解密 - 实现方法与示例讲解</title>
    <link>https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/</link>
    <pubDate>Thu, 24 Aug 2023 22:06:37 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/</guid>
    <description><![CDATA[<h2 id="bpe-简介" class="headerLink">
    <a href="#bpe-%e7%ae%80%e4%bb%8b" class="header-mark" aria-label="Header mark for 'BPE 简介'"></a>BPE 简介</h2><p>在 NLP 里面，一个核心的问题是，如何对文本进行分词？从分类的角度上面来说，可以分为：</p>
<ul>
<li>Char level</li>
<li>Word level</li>
<li>Subword level</li>
</ul>
<p>先看 Char level 分词，顾名思义，就是把文本拆分成一个个字符单独表示，<em>比如 <code>highest -&gt; h, i, g, h, e, s, t</code></em>，一个显然的好处是，Vocab 不会太大，Vocab 的大小为字符集的大小，也不会遇到 Out-of-vocabulary(OOV) 的问题，但是<strong>字符本身并没有传达太多的语义</strong>，而且<strong>分词之后会有太多的 token</strong>，<em>光是一个 highest 就可以得到 7 个 token，难以想象很长的文本分出来会有多少个</em>😨</p>]]></description>
</item><item>
    <title>TF-IDF 模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[<h2 id="什么是-tf-idf-模型" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-tf-idf-%e6%a8%a1%e5%9e%8b" class="header-mark" aria-label="Header mark for '什么是 TF-IDF 模型'"></a>什么是 TF-IDF 模型</h2><p>在之前的<a href="https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/" rel="">文章</a>中谈到了词袋模型，也讲到了它的许多不足，在今天的这篇文章中，我们要尝试解决词袋模型的<strong>缺点之一：每个词的重要性是一样的</strong></p>
<blockquote>
  <p>💡 那么，核心问题就是————<strong>如何定义「单词的重要性」这个概念</strong>？</p>

</blockquote><p>一个想法是：一个单词在<strong>一个文档</strong>里面出现得越频繁，则这个单词<strong>对于这个文档来说</strong>越重要。<em>比如一篇讨论狗的文章，大概率文章里面会出现很多「狗」，即词频高的单词反映了文档的主题</em></p>]]></description>
</item><item>
    <title>词袋模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[<h2 id="什么是词袋模型" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b" class="header-mark" aria-label="Header mark for '什么是词袋模型'"></a>什么是词袋模型</h2><p>在 NLP 中，我们需要将文档（document）表示为向量，这是因为机器学习只能够处理数字。也就是说，我们要找到下面这么一个<em>神奇</em>的函数：</p>]]></description>
</item></channel>
</rss>
