<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Deep-Learning - 标签 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/tags/deep-learning/</link>
        <description>Deep-Learning - 标签 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Thu, 12 Oct 2023 16:38:18 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/tags/deep-learning/" rel="self" type="application/rss+xml" /><item>
    <title>LLM 推理加速 - KV Cache</title>
    <link>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 16:38:18 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark" aria-label="Header mark for '背景'"></a>背景</h2><p>LLM 用于推理的时候就是不断基于前面的所有 token 生成下一个 token</p>
<p>假设现在已经生成了 $t$ 个 token，用 $x_{1:t}$ 表示。在下一轮，LLM 会生成 $x_{1:t+1}$，注意他们的前 $t$ 个 token 是<em>一样的</em></p>]]></description>
</item><item>
    <title>LoRA 微调</title>
    <link>https://martinlwx.github.io/zh-cn/lora-finetuning/</link>
    <pubDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/lora-finetuning/</guid>
    <description><![CDATA[<h2 id="什么是-lora" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-lora" class="header-mark" aria-label="Header mark for '什么是 LoRA'"></a>什么是 LoRA</h2><figure><img src="/img/lora.jpg">
</figure>

<p>自从 LLM 时代到来之后，如何微调 LLM 成为了一个难题，因为 LLM 的模型实在是太大了，很难做全量微调更新所有参数。可选的路线有：冻结整个模型做 Prompt tuning 或者 In-context Learning；冻结整个模型<em>但是</em>会插入可训练的模块。今天要介绍的 LoRA(<strong>Lo</strong>w-<strong>R</strong>ank <strong>A</strong>daptation) 就对应了后者的技术路线，这是微软团队的工作<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>]]></description>
</item><item>
    <title>机器学习求解梯度的小技巧</title>
    <link>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</link>
    <pubDate>Wed, 26 Jul 2023 00:31:50 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>也许你和我一样在求解机器学习的梯度时有各种困难，即使看着相关的 <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank" rel="noopener noreferrer">Cookbook</a> 一边推导依然是有困惑，今天我要分享的是最近<a href="https://youtu.be/JLg1HkzDsKI" target="_blank" rel="noopener noreferrer">学习到</a>的一个实用技巧：<strong>在机器学习中，求解偏导数的时候可以先全部看成标量处理，最后让维度匹配即可</strong></p>]]></description>
</item><item>
    <title>Pytorch 张量的 strides 格式是什么</title>
    <link>https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/</link>
    <pubDate>Fri, 14 Jul 2023 15:26:16 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/</guid>
    <description><![CDATA[<h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>尽管我已经使用 Numpy 和 Pytorch 好长一段时间了，但我一直不知道他们是如何实现底层的张量（tensor），而且<strong>这么高效</strong>。最近在看 <a href="https://dlsyscourse.org/" target="_blank" rel="noopener noreferrer">Deep Learning Systems</a> 这门课，终于有机会尝试自己实现张量，实现一遍之后对张量的理解更深刻了🧐</p>]]></description>
</item><item>
    <title>用 MPNN 框架解读 GAT</title>
    <link>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</link>
    <pubDate>Sun, 21 May 2023 15:20:50 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</guid>
    <description><![CDATA[<h2 id="什么是-mpnn-框架" class="headerLink">
    <a href="#%e4%bb%80%e4%b9%88%e6%98%af-mpnn-%e6%a1%86%e6%9e%b6" class="header-mark" aria-label="Header mark for '什么是 MPNN 框架'"></a>什么是 MPNN 框架</h2><p>Justin Gilmer 提出了 MPNN（Message Passing Neural Network）框架<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> ，用于描述被用来做图上的监督学习的图神经网络模型。我发现这是一个很好用的框架，可以很好理解不同的 GNN 模型是如何工作的，方便快速弄清楚不同的 GNN 模型之间的差别。我们考虑图 $G$ 上的一个节点 $v$，它的向量表示 $h_v$ 的更新方式如下：
$$m_v^{t+1}=\sum_{u\in \mathcal{N}(v)}M_t(h_v^t,h_u^t,e_{vu})$$
$$h_v^{t+1}=U_t(h_v^t,m_v^{t+1})$$</p>]]></description>
</item><item>
    <title>反向传播公式推导和理解</title>
    <link>https://martinlwx.github.io/zh-cn/backpropagation-tutorial/</link>
    <pubDate>Tue, 04 Apr 2023 13:45:48 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/backpropagation-tutorial/</guid>
    <description><![CDATA[<blockquote>
  <p>更新：矩阵形式的反向传播可以看 <a href="https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/" rel="">这里</a></p>

</blockquote><h2 id="引言" class="headerLink">
    <a href="#%e5%bc%95%e8%a8%80" class="header-mark" aria-label="Header mark for '引言'"></a>引言</h2><p>在深度学习中，模型的优化是通过采用梯度下降法不断更新权重和偏置项，让损失越来越小。其中的核心就是反向传播算法。回忆梯度下降的公式，用 $\theta$ 表示模型所有可学习的参数，$J$ 表示损失函数，$\alpha$ 表示学习率，那么有</p>]]></description>
</item></channel>
</rss>
