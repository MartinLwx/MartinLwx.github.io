<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - 标签 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/tags/llm/</link>
        <description>LLM - 标签 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Mon, 23 Dec 2024 22:01:35 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/tags/llm/" rel="self" type="application/rss+xml" /><item>
    <title>论文阅读: Generalization through Memorization: Nearest Neighbor Language Models</title>
    <link>https://martinlwx.github.io/zh-cn/what-is-knn-lm/</link>
    <pubDate>Mon, 23 Dec 2024 22:01:35 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/what-is-knn-lm/</guid>
    <description><![CDATA[<h2 id="motivation" class="headerLink">
    <a href="#motivation" class="header-mark" aria-label="Header mark for 'Motivation'"></a>Motivation</h2><p>语言模型解决 2 种问题</p>
<ol>
<li>用一个特征向量表示句子前缀</li>
<li>使用该特征向量预测下一个 token</li>
</ol>
<p>本文提出的 $k\texttt{NN-LM}$ 基于这么一个假设：<em>学习特征向量表示比预测下一个 token</em>，因此本文的方法主要基于该假设进行设计</p>]]></description>
</item><item>
    <title>论文阅读: In-Context Retrieval-Augmented Language Models</title>
    <link>https://martinlwx.github.io/zh-cn/in-context-ralm-paper-reading/</link>
    <pubDate>Wed, 04 Dec 2024 00:53:25 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/in-context-ralm-paper-reading/</guid>
    <description><![CDATA[<h2 id="the-idea" class="headerLink">
    <a href="#the-idea" class="header-mark" aria-label="Header mark for 'The idea'"></a>The idea</h2><p>In-Context RALM<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> 是用于 Autoregressive LM 上的 RAG 技术。RAG 说白了就是在模型推理的时候有个 Retriever 检索相关的文档，检索到的文档会和本来的输入拼接在一起</p>
<p>在 In-Context Learning 里面，会把一些例子放在用户输入的<em>前面</em>，再给 LLM。因此不难想象 In-Context RALM <em>也类似</em>：In-Context RALM 就是将检索到的<em>最相关</em>的文档直接拼在模型输入的<em>前面</em>，优势是<em>不需要再训练 LLM</em>，我用 mermaid 画了一个图，如下所示</p>]]></description>
</item><item>
    <title>LLM 推理加速 - KV Cache</title>
    <link>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 16:38:18 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/zh-cn/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[<h2 id="背景" class="headerLink">
    <a href="#%e8%83%8c%e6%99%af" class="header-mark" aria-label="Header mark for '背景'"></a>背景</h2><p>LLM 用于推理的时候就是不断基于前面的所有 token 生成下一个 token</p>
<p>假设现在已经生成了 $t$ 个 token，用 $x_{1:t}$ 表示。在下一轮，LLM 会生成 $x_{1:t+1}$，注意他们的前 $t$ 个 token 是<em>一样的</em></p>]]></description>
</item></channel>
</rss>
