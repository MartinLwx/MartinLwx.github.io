<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Paper - 标签 - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/zh-cn/tags/paper/</link>
        <description>Paper - 标签 - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/tags/paper/" rel="self" type="application/rss+xml" /><item>
    <title>LoRA 微调</title>
    <link>https://martinlwx.github.io/zh-cn/lora-finetuning/</link>
    <pubDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/lora-finetuning/</guid>
    <description><![CDATA[什么是 LoRA 自从 LLM 时代到来之后，如何微调 LLM 成为了一个难题，因为 LLM 的模型实在是太大了，很难做全量微调更新所有参数。可选的路线有：冻结整个模型做 Prompt tuning]]></description>
</item>
<item>
    <title>用 MPNN 框架解读 GAT</title>
    <link>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</link>
    <pubDate>Sun, 21 May 2023 15:20:50 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</guid>
    <description><![CDATA[什么是 MPNN 框架 Justin Gilmer 提出了 MPNN（Message Passing Neural Network）框架1 ，用于描述被用来做图上的监督学习的图神经网络模型。我发现这是一个很好]]></description>
</item>
</channel>
</rss>
