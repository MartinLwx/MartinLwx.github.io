<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>所有文章 - MartinLwx&#39;s blog</title>
        <link>https://martinlwx.github.io/zh-cn/posts/</link>
        <description>所有文章 | MartinLwx&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><lastBuildDate>Wed, 06 Sep 2023 23:13:52 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/zh-cn/posts/" rel="self" type="application/rss+xml" /><item>
    <title>下一个排列问题</title>
    <link>https://martinlwx.github.io/zh-cn/the-next-lexicographical-permutation-problem/</link>
    <pubDate>Wed, 06 Sep 2023 23:13:52 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/the-next-lexicographical-permutation-problem/</guid>
    <description><![CDATA[引言 有时候我们会想要生成一个序列的「下一个排列」或者是「上一个排列」，你会怎么做呢？如果你对 C++ 很熟悉的话，不难想到可以用 next_permutation1 和 prev_per]]></description>
</item>
<item>
    <title>BPE 分词解密 - 实现方法与示例讲解</title>
    <link>https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/</link>
    <pubDate>Thu, 24 Aug 2023 22:06:37 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/the-bpe-tokenizer/</guid>
    <description><![CDATA[BPE 简介 在 NLP 里面，一个核心的问题是，如何对文本进行分词？从分类的角度上面来说，可以分为： Char level Word level Subword level 先看 Char level 分词，顾名思义，就是把文本拆分成一]]></description>
</item>
<item>
    <title>TF-IDF 模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[什么是 TF-IDF 模型 在之前的文章中谈到了词袋模型，也讲到了它的许多不足，在今天的这篇文章中，我们要尝试解决词袋模型的缺点之一：每个词的重要性是一样的]]></description>
</item>
<item>
    <title>词袋模型</title>
    <link>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[什么是词袋模型 在 NLP 中，我们需要将文档（document）表示为向量，这是因为机器学习只能够处理数字。也就是说，我们要找到下面这么一个神奇的函]]></description>
</item>
<item>
    <title>机器学习求解梯度的小技巧</title>
    <link>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</link>
    <pubDate>Wed, 26 Jul 2023 00:31:50 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/a-trick-to-calculating-partial-derivatives-in-ml/</guid>
    <description><![CDATA[引言 也许你和我一样在求解机器学习的梯度时有各种困难，即使看着相关的 Cookbook 一边推导依然是有困惑，今天我要分享的是最近学习到的一个实用技巧：在机器学]]></description>
</item>
<item>
    <title>Pytorch 张量的 strides 格式是什么</title>
    <link>https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/</link>
    <pubDate>Fri, 14 Jul 2023 15:26:16 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/how-to-reprensent-a-tensor-or-ndarray/</guid>
    <description><![CDATA[引言 尽管我已经使用 Numpy 和 Pytorch 好长一段时间了，但我一直不知道他们是如何实现底层的张量（tensor），而且这么高效。最近在看 Deep Learning Systems 这门课，终于有机]]></description>
</item>
<item>
    <title>如何记忆红黑树的操作</title>
    <link>https://martinlwx.github.io/zh-cn/how-to-memorize-insertion-and-deletion-in-rb-tree/</link>
    <pubDate>Sat, 01 Jul 2023 17:12:40 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/how-to-memorize-insertion-and-deletion-in-rb-tree/</guid>
    <description><![CDATA[引言 如果你点进了这一篇文章，相信你也跟我一样：红黑树学一次忘一次，又要做树的旋转，又要给节点重新上色，导致每次都是学完了就忘记。我也曾经仔细]]></description>
</item>
<item>
    <title>Git Bundle 指南</title>
    <link>https://martinlwx.github.io/zh-cn/git-bundle-tutorial/</link>
    <pubDate>Fri, 16 Jun 2023 23:48:28 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/git-bundle-tutorial/</guid>
    <description><![CDATA[git bundle 是什么 git bundle 是一个比较少看到的 git 命令，它的作用是把一个 git 仓库打包📦成一个文件，然后别人可以通过这个文件还原出本来的 git 仓库，而且 git bundle 还支持增]]></description>
</item>
<item>
    <title>用 MPNN 框架解读 GAT</title>
    <link>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</link>
    <pubDate>Sun, 21 May 2023 15:20:50 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/understanding-graph-attention-network-through-mpnn/</guid>
    <description><![CDATA[什么是 MPNN 框架 Justin Gilmer 提出了 MPNN（Message Passing Neural Network）框架1 ，用于描述被用来做图上的监督学习的图神经网络模型。我发现这是一个很好]]></description>
</item>
<item>
    <title>SICP 练习 2.27</title>
    <link>https://martinlwx.github.io/zh-cn/sicp-exercise-2-27/</link>
    <pubDate>Tue, 16 May 2023 12:41:20 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/zh-cn/sicp-exercise-2-27/</guid>
    <description><![CDATA[题目 Modify your reverse procedure of exercise 2.18 to produce a deep-reverse procedure that taks a list as argument and returns as its value the list with its elements reversed and with all sublists deep-reversed as well. 1 2 3 4 (define x (list (list 1 2) (list 3 4))) ;; x - ((1 2) (3 4)) (deep-reverse x) ;; the output should be ((4 3) (2 1)) 答]]></description>
</item>
</channel>
</rss>
