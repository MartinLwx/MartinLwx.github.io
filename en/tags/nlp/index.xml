<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>NLP - Tag - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/tags/nlp/</link>
        <description>NLP - Tag - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Thu, 12 Oct 2023 18:29:31 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>LLM inference optimization - KV Cache</title>
    <link>https://martinlwx.github.io/en/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 18:29:31 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/en/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[Background The secret behind LLM is that it will generate tokens one by one based on all the previous tokens.
Let&rsquo;s assume that we have already generated $t$ tokens, denoted by $x_{1:t}$. In the next iteration, the LLM will generate $x_{1:t+1}$. Note that the first $t$ tokens are the same.
$$x_{1:t+1}=\text{LLM}(x_{1:t})$$
The next iteration is similar.
$$x_{1:t+2}=\text{LLM}(x_{1:t+1})$$
In summary, in each iteration, we will use the output of the previous round as a new input for the LLM.]]></description>
</item>
<item>
    <title>BPE Tokenization Demystified: Implementation and Examples</title>
    <link>https://martinlwx.github.io/en/the-bpe-tokenizer/</link>
    <pubDate>Thu, 24 Aug 2023 22:06:37 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/en/the-bpe-tokenizer/</guid>
    <description><![CDATA[A taxonomy of tokenization methods In NLP, one crux of problems is - how to tokenize the text. There are three methods available:
Char-level Word-level Subword-level Let&rsquo;s talk about the Char-level tokenizer. That is, we tokenize the text into a char stream. For instance, highest -&gt; h, i, g, h, e, s, t. One advantage of the Char-level tokenizer is that the size of Vocab won&rsquo;t be that large. The size of Vocab is equal to the size of the alphabet.]]></description>
</item>
<item>
    <title>TF-IDF model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[What is the TF-IDF model In previous post, we talked about the bag-of-word model, which has many limitations. Today we take a step further to see if we can try to fix one of the limitations - Each word has the same importance.
ðŸ’¡ The crux of the problem - How to define the word importance?
One idea is: The more frequently a word appears within a single document, the more important it is for that document.]]></description>
</item>
<item>
    <title>Bag-of-Word model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate><author>martinlwx@163.com (MartinLwx)</author><guid>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[What is the bag-of-word model? In NLP, we need to represent each document as a vector because machine learning can only accept input as numbers. That is, we want to find a magic function that: $$ f(\text{document}) = vector $$
Today&rsquo;s topic is bag-of-word(BoW) model, which can transform a document into a vector representation.
ðŸ’¡ Although the BoW model is outdated in 2023, I still encourage you to learn from the history and think about some essential problems:]]></description>
</item>
</channel>
</rss>
