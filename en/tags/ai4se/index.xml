<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>AI4SE - Tag - MartinLwx&#39;s blog</title>
        <link>https://martinlwx.github.io/en/tags/ai4se/</link>
        <description>AI4SE - Tag - MartinLwx&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><lastBuildDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/tags/ai4se/" rel="self" type="application/rss+xml" /><item>
    <title>Bag-of-Word model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[What is the bag-of-word model? In NLP, we need to represent each document as a vector because machine learning can only accept input as numbers. That is, we want to find a magic function that: $$ f(\text{document}) = vector $$
Today&rsquo;s topic is bag-of-word(BoW) model, which can transform a document into a vector representation.
ðŸ’¡ Although the BoW model is outdated in 2023, I still encourage you to learn from the history and think about some essential problems:]]></description>
</item>
</channel>
</rss>
