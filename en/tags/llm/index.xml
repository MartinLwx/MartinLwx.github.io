<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LLM - Tag - MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/tags/llm/</link>
        <description>LLM - Tag - MartinLwx&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Thu, 12 Oct 2023 18:29:31 &#43;0800</lastBuildDate><atom:link href="https://martinlwx.github.io/en/tags/llm/" rel="self" type="application/rss+xml" /><item>
    <title>LLM inference optimization - KV Cache</title>
    <link>https://martinlwx.github.io/en/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 18:29:31 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[<h2 id="background" class="headerLink">
    <a href="#background" class="header-mark" aria-label="Header mark for 'Background'"></a>Background</h2><p>The secret behind LLM is that it will generate tokens one by one based on all the previous tokens.</p>
<p><em>Let&rsquo;s assume that we have already generated $t$ tokens, denoted by $x_{1:t}$. In the next iteration, the LLM will generate $x_{1:t+1}$. Note that the first $t$ tokens are the same</em>.</p>
<p>$$x_{1:t+1}=\text{LLM}(x_{1:t})$$</p>
<p><em>The next iteration is similar.</em></p>
<p>$$x_{1:t+2}=\text{LLM}(x_{1:t+1})$$</p>
<p>In summary, in each iteration, we will <em>use the output of the previous round</em> as a new input for the LLM. Generally, this process will continue until the output reaches the maximum length we predefined or the LLM itself generates a special token, signifying the completion of the generating process.</p>]]></description>
</item></channel>
</rss>
