<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MartinLwx&#39;s Blog</title>
        <link>https://martinlwx.github.io/en/</link>
        <description>Welcome to my blog :)</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><copyright>&lt;a rel=&#34;license noopener&#34; href=&#34;https://creativecommons.org/licenses/by-nc-nd/4.0/&#34; target=&#34;_blank&#34;&gt;CC BY-NC-ND 4.0&lt;/a&gt;</copyright><lastBuildDate>Mon, 29 Jul 2024 23:35:53 &#43;0800</lastBuildDate>
            <atom:link href="https://martinlwx.github.io/en/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Use Github Actions to automate Hugo blog deployment</title>
    <link>https://martinlwx.github.io/en/use-github-actions-to-automated-hugo-build/</link>
    <pubDate>Mon, 29 Jul 2024 23:35:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/use-github-actions-to-automated-hugo-build/</guid>
    <description><![CDATA[IntroRecently I started learning the GitHub Actions, a feature provided by GitHub that can be used to automate a series of steps. In the process of software development, the most common use of this feature may be the building process. For static-typed programming languages such as C/C++, we are usually required to write the build scripts. The build process involves environment preparation, dependencies download, and build execution. However, automating software builds with GitHub Actions is not the focus of this post.]]></description>
</item><item>
    <title>Tail call and Tail-call Optimization (TCO)</title>
    <link>https://martinlwx.github.io/en/tail-call-and-tail-call-optimization/</link>
    <pubDate>Fri, 22 Mar 2024 12:13:53 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/tail-call-and-tail-call-optimization/</guid>
    <description><![CDATA[Tail call &amp; Tail-recursionAssume that function A calls function B. We call function A Caller and function B Callee.
The tail call refers to when the Caller only needs to wait for the return value of the Callee, as everything else has already been completed1.
If this is a recursive function call(e.g. the Caller and Callee are the same function), then the function is said to be tail-recursive.
Tail-call Optimization (TCO)According to the mechanism of the function call, a stack frame will be created to store some useful information such as the return address.]]></description>
</item><item>
    <title>Learn to use text-object in Vim&amp;Neovim</title>
    <link>https://martinlwx.github.io/en/learn-to-use-text-objects-in-vim/</link>
    <pubDate>Sun, 03 Mar 2024 19:46:48 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/learn-to-use-text-objects-in-vim/</guid>
    <description><![CDATA[IntroYou probably do not know what the text-object is in Vim/Neovim. However, you may use it in your daily life. For instance, When you are writing code, you may want to change the arguments of a function. Take the following code as an example, let&rsquo;s say you want to change the function call to bar(3, 2, 1), and the cursor currently stays on the ,
1 2 3 4 5 6 def foo(): .]]></description>
</item><item>
    <title>Neovim Setup for OCaml</title>
    <link>https://martinlwx.github.io/en/neovim-setup-for-ocaml/</link>
    <pubDate>Tue, 23 Jan 2024 12:55:46 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/neovim-setup-for-ocaml/</guid>
    <description><![CDATA[Neovim Setup for OCaml]]></description>
</item><item>
    <title>LLM inference optimization - KV Cache</title>
    <link>https://martinlwx.github.io/en/llm-inference-optimization-kv-cache/</link>
    <pubDate>Thu, 12 Oct 2023 18:29:31 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/llm-inference-optimization-kv-cache/</guid>
    <description><![CDATA[BackgroundThe secret behind LLM is that it will generate tokens one by one based on all the previous tokens.
Let&rsquo;s assume that we have already generated $t$ tokens, denoted by $x_{1:t}$. In the next iteration, the LLM will generate $x_{1:t+1}$. Note that the first $t$ tokens are the same.
$$x_{1:t+1}=\text{LLM}(x_{1:t})$$
The next iteration is similar.
$$x_{1:t+2}=\text{LLM}(x_{1:t+1})$$
In summary, in each iteration, we will use the output of the previous round as a new input for the LLM.]]></description>
</item><item>
    <title>LoRA fine-tuning</title>
    <link>https://martinlwx.github.io/en/lora-finetuning/</link>
    <pubDate>Thu, 14 Sep 2023 22:57:06 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/lora-finetuning/</guid>
    <description><![CDATA[What&rsquo;s LoRA Since the era of LLM(large language model) arrived, fine-tuning LLM has become a challenge because the LLM models are extremely large, making it difficult to perform full fine-tuning. There are mainly two approaches: freeze the entire LLM and perform prompt tuning or In-context Learning; freeze the entire LLM but inserting trainable modules. Today, I will introduce the LoRA(Low-Rank Adaptation), which corresponds to the latter technical approach. This is a work proposed by the Microsoft team1]]></description>
</item><item>
    <title>The next lexicographical permutation problem</title>
    <link>https://martinlwx.github.io/en/the-next-lexicographical-permutation-problem/</link>
    <pubDate>Wed, 06 Sep 2023 23:13:52 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/the-next-lexicographical-permutation-problem/</guid>
    <description><![CDATA[IntroOccasionally, you may want to get the next/prev lexicographical permutation of a sequence. How would you do that? If you are a C++ programmer, you are probably familiar with the next_permutation1 and prev_permutation2 APIs. However, Python does not provide the counterparts. So the topic today is how to do this in Python. Since the solutions of prev lexicographical permutation and the next lexicographical permutation are very similar, let us focus on the next lexicographical permutation problem.]]></description>
</item><item>
    <title>BPE Tokenization Demystified: Implementation and Examples</title>
    <link>https://martinlwx.github.io/en/the-bpe-tokenizer/</link>
    <pubDate>Thu, 24 Aug 2023 22:06:37 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/the-bpe-tokenizer/</guid>
    <description><![CDATA[A taxonomy of tokenization methodsIn NLP, one crux of problems is - how to tokenize the text. There are three methods available:
Char-level Word-level Subword-level Let&rsquo;s talk about the Char-level tokenizer. That is, we tokenize the text into a char stream. For instance, highest -&gt; h, i, g, h, e, s, t. One advantage of the Char-level tokenizer is that the size of Vocab won&rsquo;t be that large. The size of Vocab is equal to the size of the alphabet.]]></description>
</item><item>
    <title>TF-IDF model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</link>
    <pubDate>Wed, 16 Aug 2023 22:23:26 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-introduction-of-tf-idf-model/</guid>
    <description><![CDATA[What is the TF-IDF modelIn previous post, we talked about the bag-of-word model, which has many limitations. Today we take a step further to see if we can try to fix one of the limitations - Each word has the same importance.
ðŸ’¡ The crux of the problem - How to define the word importance?
One idea is: The more frequently a word appears within a single document, the more important it is for that document.]]></description>
</item><item>
    <title>Bag-of-Word model</title>
    <link>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</link>
    <pubDate>Fri, 11 Aug 2023 18:55:09 &#43;0800</pubDate><author>
        <name>MartinLwx</name>
    </author><guid>https://martinlwx.github.io/en/an-introduction-of-bag-of-word-model/</guid>
    <description><![CDATA[What is the bag-of-word model?In NLP, we need to represent each document as a vector because machine learning can only accept input as numbers. That is, we want to find a magic function that: $$ f(\text{document}) = vector $$
Today&rsquo;s topic is bag-of-word(BoW) model, which can transform a document into a vector representation.
ðŸ’¡ Although the BoW model is outdated in 2023, I still encourage you to learn from the history and think about some essential problems:]]></description>
</item></channel>
</rss>
