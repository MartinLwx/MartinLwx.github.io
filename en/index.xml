<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>MartinLwx&#39;s blog</title>
        <link>https://martinlwx.github.io/en/</link>
        <description>Welcome to my blog :)</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>martinlwx@163.com (MartinLwx)</managingEditor>
            <webMaster>martinlwx@163.com (MartinLwx)</webMaster><lastBuildDate>Sun, 21 May 2023 15:20:50 &#43;0800</lastBuildDate>
            <atom:link href="https://martinlwx.github.io/en/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Understanding GAT throught MPNN</title>
    <link>https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/</link>
    <pubDate>Sun, 21 May 2023 15:20:50 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/understanding-graph-attention-network-through-mpnn/</guid>
    <description><![CDATA[What&rsquo;s MPNN Justin Gilmer proposed the MPNN (Message Passing Neural Network) framework 1 for describing graph neural network models used in supervised learning on graphs. I found this to be a useful framework that provides a clear understanding of how different GNN models work and facilitates a quick grasp of the differences between them. Considering a node $v$ on the graph $G$, the update procedure for its vector representation $h_v$ is as follows:]]></description>
</item>
<item>
    <title>SICP Exercise 2.27</title>
    <link>https://martinlwx.github.io/en/sicp-exercise-2-27/</link>
    <pubDate>Tue, 16 May 2023 12:41:20 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/sicp-exercise-2-27/</guid>
    <description><![CDATA[Question Modify your reverse procedure of exercise 2.18 to produce a deep-reverse procedure that takes a list as an argument and returns as its value the list with its elements reversed and with all sublists deep-reversed as well.
1 2 3 4 (define x (list (list 1 2) (list 3 4))) ;; x - ((1 2) (3 4)) (deep-reverse x) ;; the output should be ((4 3) (2 1)) Answer In the previous Exercise 2.]]></description>
</item>
<item>
    <title>SICP Exercise 1.46</title>
    <link>https://martinlwx.github.io/en/sicp-exercise-1-46/</link>
    <pubDate>Wed, 10 May 2023 13:40:48 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/sicp-exercise-1-46/</guid>
    <description><![CDATA[Question Several of the numerical methods described in this chapter are instances of an extremely general computational strategy known as iterative improvement. Iterative improvement says that, to compute something, we start with an initial guess for the answer, test if the guess is good enough, and otherwise improve the guess and continue the process using the improved guess as the new guess. Write a procedure iterative-improve that takes two procedures as arguments: a method for telling whether a guess is good enough and a method for improving a guess.]]></description>
</item>
<item>
    <title>SICP Exercise 1.34</title>
    <link>https://martinlwx.github.io/en/sicp-exercise-1-34/</link>
    <pubDate>Tue, 09 May 2023 14:06:18 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/sicp-exercise-1-34/</guid>
    <description><![CDATA[Question Suppose we define the procedure f. What happens if we (perversely) ask the interpreter to evaluate the combination (f f)?
1 2 3 4 5 (define (square x) (* x x)) (define (f g) (g 2)) Then we have
1 2 3 (f square) ;; 4 (f (lambda (z) (* z (+ z 1)))) ;; 6 = 2 * 3 Answer Recall what the applicative-order evaluation says: We need to evaluate all arguments and then we apply procedure on these arguments.]]></description>
</item>
<item>
    <title>Solution of Proj4. Scheme Interpreter of CS61A (2021-Fall)</title>
    <link>https://martinlwx.github.io/en/proj4-scheme-interpreter-of-cs61a-of-ucb-fa21/</link>
    <pubDate>Fri, 21 Apr 2023 10:18:16 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/proj4-scheme-interpreter-of-cs61a-of-ucb-fa21/</guid>
    <description><![CDATA[Intro Recently, I am reading a book called Crafting interpreters written by Robert Nystrom. In the original book, a Tree-walker interpreter jlox was implemented in Java. And I am trying to rewrite in Python - pylox. I highly recommend it👍. At this moment, I suddenly remembered that there were a few small issues with the Scheme interpreter for CS61A that I had not resolved after finishing it a year ago, which kept it in an unfinished state.]]></description>
</item>
<item>
    <title>Solving DP problems by SRTBOT Framework</title>
    <link>https://martinlwx.github.io/en/solving-dynamic-programming-problems-using-srtbot/</link>
    <pubDate>Sun, 09 Apr 2023 12:30:31 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/solving-dynamic-programming-problems-using-srtbot/</guid>
    <description><![CDATA[Changelog:
Update dependency graphs @2023.04.13 Intro When solving algorithm problems, what often gives me a headache are dynamic programming problems(DP problems). They are the type of problems that I can&rsquo;t figure out on my own after thinking for a long time, but after seeing the answer, it suddenly becomes clear and reasonable. However, the next time I encounter a similar problem, I may forget how to solve it. I have also read many people&rsquo;s solutions and tried to digest and apply their ideas, but I have been unable to find a particularly good framework that works for all dynamic programming problems.]]></description>
</item>
<item>
    <title>How to understand the backpropagation algorithm</title>
    <link>https://martinlwx.github.io/en/backpropagation-tutorial/</link>
    <pubDate>Tue, 04 Apr 2023 13:45:48 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/backpropagation-tutorial/</guid>
    <description><![CDATA[Intro In the field of deep learning, optimizing the network involves a crucial process of continuously updating the weights and bias items. This is achieved by implementing the gradient descent method, which progressively minimizes the loss function. At the heart of this process lies the backpropagation algorithm, which facilitates efficient computation of gradients across the network
To better understand this concept, let us recall the formula for gradient descent. In this formula, we utilize the symbol $\theta$ to represent all the learnable parameters of the model, $J$ to represent the cost or loss function, and $\alpha$ to denote the learning rate.]]></description>
</item>
<item>
    <title>Linear Regression Model Guide - theory part</title>
    <link>https://martinlwx.github.io/en/linear-regression-model-guide-theory/</link>
    <pubDate>Wed, 15 Mar 2023 12:37:52 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/linear-regression-model-guide-theory/</guid>
    <description><![CDATA[Introduction Recently, I review the machine learning course of Andrew ng in Coursera. Surprisingly, I can still learn a lot, so I decided to write some posts👍.
To talk about linear regression, we must first have a basic understanding of what is machine learning. What is machine learning? abstractly speaking, machine learning is learning a function: $$ f(input) = output $$ where $f$ refers to the specific machine learning model. Machine learning is a methodology for automatically mining the relationship between input and output.]]></description>
</item>
<item>
    <title>Transform Your Neovim into a IDE: A Step-by-Step Guide</title>
    <link>https://martinlwx.github.io/en/config-neovim-from-scratch/</link>
    <pubDate>Wed, 08 Feb 2023 15:01:42 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/config-neovim-from-scratch/</guid>
    <description><![CDATA[Versions info I use a Macbook pro-2020 Intel Edition with macOS 13.2. This is my Nvim edition:
1 2 3 4 5 6 7 8 9 10 11 12 NVIM v0.8.3 Build type: Release LuaJIT 2.1.0-beta3 Compiled by brew@Ventura Features: +acl +iconv +tui See &#34;:help feature-compile&#34; system vimrc file: &#34;$VIM/sysinit.vim&#34; fall-back for $VIM: &#34;/usr/local/Cellar/neovim/0.8.3/share/nvim&#34; Run :checkhealth for more info Why Neovim After using Vim for one year, I find myself having trouble in configure ~/.]]></description>
</item>
<item>
    <title>Type hints: what and why</title>
    <link>https://martinlwx.github.io/en/type-hints-in-python/</link>
    <pubDate>Fri, 13 Jan 2023 16:18:02 &#43;0800</pubDate>
    <author>MartinLwx</author>
    <guid>https://martinlwx.github.io/en/type-hints-in-python/</guid>
    <description><![CDATA[Intro I was immediately drawn to Python when I first encountered it due to its dynamic language features. Python use the &ldquo;duck typing&rdquo; design, which means that the type of an object is not as important as its behavior. This feature allows for faster development and a reduction in burdensome type declarations. Additionally, the support of powerful third-party libraries solidifies Python as my preferred programming language.😺
However, with the proposal of PEP 4841, Python decided to introduce type hints, which seem to be in line with statically typed languages.]]></description>
</item>
</channel>
</rss>
